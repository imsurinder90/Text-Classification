{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification using Tensorflow DNN with onehot encoding\n",
    "\n",
    "We have yelp review data taken from kaggle competetion.\n",
    "\n",
    "** Goal: **\n",
    "Predict stars ranges from 1 - 5 based on review text\n",
    "\n",
    "- The data is splitted into training and test data set.\n",
    "- Each review is tokenized and followed by stemming.\n",
    "- A list of vocabulary is generated.\n",
    "- OneHot encoded vector is generated for each review.\n",
    "- Apply the tensorflow DNN neural network model on review text.\n",
    "- Accuracy, Confusion matrix and classification report for the predicted test data is calculated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp = pd.read_csv('./data/yelp.csv')\n",
    "yelp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out text data \n",
    "X = yelp.text.apply(lambda x: re.sub(\"[^a-zA-Z]\", \" \", x))\n",
    "y = yelp.stars\n",
    "\n",
    "# find out unique list of stars\n",
    "categories = sorted(list(y_test.value_counts().keys()))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test data set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test , y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nl\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# instantiate a LancasterStemmer method\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe 'train' with columns text and its tokens\n",
    "list_tokens = [nl.word_tokenize(sentences) for sentences in X_train]\n",
    "train = pd.DataFrame({'text': X_train, 'tokens': list_tokens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of vocabolary\n",
    "words = [stemmer.stem(str(word)) for word_list in list_tokens for word in word_list]\n",
    "words = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data, we are feeding onehot encoded data to dnn model\n",
    "training = []\n",
    "for index, item in train.iterrows():\n",
    "    token_words = [stemmer.stem(word) for word in item['tokens']]\n",
    "    onehot = []\n",
    "    for word in words:\n",
    "        onehot.append(1) if word in token_words else onehot.append(0)\n",
    "    training.append([onehot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7500"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_encoded[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n"
     ]
    }
   ],
   "source": [
    "# invert first example\n",
    "inverted = label_encoder.inverse_transform(\n",
    "    [np.argmax(onehot_encoded[1, :])]\n",
    ")\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert training data to numpy array\n",
    "training_new = np.array(training)\n",
    "\n",
    "train_x = training_new[:, 0]\n",
    "train_y = onehot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 9379  | total loss: \u001b[1m\u001b[32m0.13896\u001b[0m\u001b[0m | time: 5.115s\n",
      "| Adam | epoch: 010 | loss: 0.13896 - acc: 0.9796 -- iter: 7496/7500\n",
      "Training Step: 9380  | total loss: \u001b[1m\u001b[32m0.12892\u001b[0m\u001b[0m | time: 5.120s\n",
      "| Adam | epoch: 010 | loss: 0.12892 - acc: 0.9817 -- iter: 7500/7500\n",
      "--\n",
      "INFO:tensorflow:D:\\surinder\\ds\\kaggle\\yelp_one_hot_encoding\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "\n",
    "# reset underlying graph data\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Build neural network\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    " \n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(train_x, train_y, n_epoch=10, batch_size=8, show_metric=True)\n",
    "model.save('model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe 'test' with test data and tokens for review text\n",
    "test_tokens = [nl.word_tokenize(sentences) for sentences in X_test]\n",
    "test = pd.DataFrame({'text': X_test, 'tokens': test_tokens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert test data into onehot encoded data\n",
    "testing = []\n",
    "for index,item in test.iterrows():\n",
    "    onehot = []\n",
    "    token_words = [stemmer.stem(word) for word in item['tokens']]\n",
    "    for w in words:\n",
    "        onehot.append(1) if w in token_words else onehot.append(0)\n",
    "    \n",
    "    testing.append(onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is necessary to convert list to np.array, models accepts an numpy array\n",
    "testing = list(np.array(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X=testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map predicted data with categories\n",
    "# np.argmax(x) returns either 0, 1, 2, 3, 4 but categories\n",
    "# data is [1, 2, 3, 4, 5], so 0 maps to 1, 1 to 2 so on..\n",
    "predicted_new = pd.Series(list(predicted)).apply(lambda x: categories[np.argmax(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4804"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy for test data\n",
    "metrics.accuracy_score(y_test, predicted_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 88  41  33  11  12]\n",
      " [ 38  71  77  35  13]\n",
      " [ 12  49 132 124  48]\n",
      " [ 14  33 130 425 282]\n",
      " [ 24  23  62 238 485]]\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, predicted_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      " [[  0   0   0   0   0   0]\n",
      " [ 88  41  33  11  12   0]\n",
      " [ 38  71  77  35  13   0]\n",
      " [ 12  49 132 124  48   0]\n",
      " [ 14  33 130 425 282   0]\n",
      " [ 24  23  62 238 485   0]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix using tensorflow function\n",
    "labels = list(y_test)\n",
    "predictions = [np.argmax(x) for x in predicted]\n",
    "confusion_matrix = tf.confusion_matrix(labels, predictions)\n",
    "\n",
    "with tf.Session():\n",
    "    print('\\nConfusion Matrix:\\n', tf.Tensor.eval(confusion_matrix,feed_dict=None, session=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.50      0.48      0.49       185\n",
      "          2       0.33      0.30      0.31       234\n",
      "          3       0.30      0.36      0.33       365\n",
      "          4       0.51      0.48      0.50       884\n",
      "          5       0.58      0.58      0.58       832\n",
      "\n",
      "avg / total       0.48      0.48      0.48      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate classification report\n",
    "print(metrics.classification_report(y_test, predicted_new))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}


